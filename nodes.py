from transformers import AutoModelForImageSegmentation
import torch
from torchvision import transforms
import numpy as np
from PIL import Image
import torch.nn.functional as F
import comfy.model_management as mm
import os
import gc
import weakref

torch.set_float32_matmul_precision(["high", "highest"][0])

transform_image = transforms.Compose(
    [
        transforms.Resize((1024, 1024)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),
    ]
)

current_path  = os.getcwd()

## ComfyUI portable standalone build for Windows 
model_path = os.path.join(current_path, "ComfyUI"+os.sep+"models"+os.sep+"BiRefNet")

# å…¨å±€æ¨¡å‹ç¼“å­˜å­—å…¸
_model_cache = {}
_device_cache = {}

def get_cached_model(model_name, local_model_path, load_local_model, device):
    """è·å–ç¼“å­˜çš„æ¨¡å‹ï¼Œé¿å…é‡å¤åŠ è½½"""
    cache_key = f"{model_name}_{local_model_path}_{load_local_model}"
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜çš„æ¨¡å‹
    if cache_key in _model_cache:
        model = _model_cache[cache_key]
        # æ£€æŸ¥è®¾å¤‡æ˜¯å¦åŒ¹é…
        if cache_key in _device_cache and _device_cache[cache_key] == device:
            return model
        else:
            # è®¾å¤‡ä¸åŒ¹é…ï¼Œç§»åŠ¨æ¨¡å‹åˆ°æ–°è®¾å¤‡
            model = model.to(device)
            _device_cache[cache_key] = device
            return model
    
    # åŠ è½½æ–°æ¨¡å‹
    print(f"\033[93mæ­£åœ¨åŠ è½½BiRefNetæ¨¡å‹åˆ°è®¾å¤‡: {device}\033[0m")
    
    # åœ¨åŠ è½½æ–°æ¨¡å‹å‰ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿæ˜¾å­˜
    if device.startswith('cuda'):
        torch.cuda.empty_cache()
        gc.collect()
    
    if load_local_model:
        model = AutoModelForImageSegmentation.from_pretrained(
            local_model_path, trust_remote_code=True
        )
    else:
        model = AutoModelForImageSegmentation.from_pretrained(
            model_name, trust_remote_code=True
        )
    
    # ä½¿ç”¨ComfyUIçš„æ˜¾å­˜ç®¡ç†
    model = model.to(device)
    
    # ç¼“å­˜æ¨¡å‹
    _model_cache[cache_key] = model
    _device_cache[cache_key] = device
    
    return model

def clear_model_cache():
    """æ¸…ç†æ¨¡å‹ç¼“å­˜"""
    global _model_cache, _device_cache
    for model in _model_cache.values():
        del model
    _model_cache.clear()
    _device_cache.clear()
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

def tensor2pil(image):
    return Image.fromarray(np.clip(255. * image.cpu().numpy().squeeze(), 0, 255).astype(np.uint8))

def pil2tensor(image):
    return torch.from_numpy(np.array(image).astype(np.float32) / 255.0).unsqueeze(0)

def resize_image(image, max_size=1024):
    """æ™ºèƒ½resizeï¼Œé¿å…è¿‡å¤§çš„å¼ é‡"""
    image = image.convert('RGB')
    w, h = image.size
    
    # å¦‚æœå›¾ç‰‡å·²ç»å¾ˆå°ï¼Œä¸éœ€è¦resizeåˆ°1024
    if max(w, h) <= max_size:
        # ä¿æŒåŸå§‹å°ºå¯¸ï¼Œä½†ç¡®ä¿æ˜¯å¶æ•°
        w = w if w % 2 == 0 else w + 1
        h = h if h % 2 == 0 else h + 1
        model_input_size = (w, h)
    else:
        # ç­‰æ¯”ç¼©æ”¾åˆ°max_size
        if w > h:
            new_w = max_size
            new_h = int(h * max_size / w)
        else:
            new_h = max_size
            new_w = int(w * max_size / h)
        
        # ç¡®ä¿å°ºå¯¸æ˜¯å¶æ•°
        new_w = new_w if new_w % 2 == 0 else new_w + 1
        new_h = new_h if new_h % 2 == 0 else new_h + 1
        model_input_size = (new_w, new_h)
    
    image = image.resize(model_input_size, Image.BILINEAR)
    return image

colors = ["transparency", "green", "white", "red", "yellow", "blue", "black", "pink", "purple", "brown", "violet", "wheat", "whitesmoke", "yellowgreen", "turquoise", "tomato", "thistle", "teal", "tan", "steelblue", "springgreen", "snow", "slategrey", "slateblue", "skyblue", "orange"]

def get_device_by_name(device):
    """
    "device": (["auto", "cuda", "cpu", "mps", "xpu", "meta"],{"default": "auto"}), 
    """
    if device == 'auto':
        try:
            device = "cpu"
            if torch.cuda.is_available():
                device = "cuda"
            elif torch.backends.mps.is_available():
                device = "mps"
            elif torch.xpu.is_available():
                device = "xpu"
        except:
                raise AttributeError("What's your device(åˆ°åº•ç”¨ä»€ä¹ˆè®¾å¤‡è·‘çš„)ï¼Ÿ")
    print("\033[93mUse Device(ä½¿ç”¨è®¾å¤‡):", device, "\033[0m")
    return device


class BiRefNet_Hugo:
    def __init__(self):
        pass

    @classmethod
    def INPUT_TYPES(cls):
        
        return {
            "required": {
                "image": ("IMAGE",),
                "model": (["ZhengPeng7/BiRefNet", "ZhengPeng7/BiRefNet_HR", "ZhengPeng7/BiRefNet-portrait"],{"default": "ZhengPeng7/BiRefNet"}),
                "load_local_model": ("BOOLEAN", {"default": False}),
                "background_color_name": (colors,{"default": "transparency"}), 
                "device": (["auto", "cuda", "cpu", "mps", "xpu", "meta"],{"default": "auto"}),
                "max_resolution": ("INT", {"default": 1024, "min": 512, "max": 2048, "step": 64}),
                "enable_memory_efficient": ("BOOLEAN", {"default": True}),
            },
            "optional": {
                "local_model_path": ("STRING", {"default":model_path}),
            }
        }

    RETURN_TYPES = ("IMAGE", "MASK",)
    RETURN_NAMES = ("image", "mask",)
    FUNCTION = "background_remove"
    CATEGORY = "ğŸ”¥BiRefNet"
  
    def background_remove(self, 
                          image, 
                          model,
                          load_local_model,
                          device, 
                          background_color_name,
                          max_resolution=1024,
                          enable_memory_efficient=True,
                          *args, **kwargs
                          ):
        processed_images = []
        processed_masks = []
       
        device = get_device_by_name(device)
        
        try:
            # è·å–ç¼“å­˜çš„æ¨¡å‹
            local_model_path = kwargs.get("local_model_path", model_path)
            birefnet = get_cached_model(model, local_model_path, load_local_model, device)
            
            # å¦‚æœå¯ç”¨å†…å­˜æ•ˆç‡æ¨¡å¼ï¼Œåˆ†æ‰¹å¤„ç†
            batch_size = 1 if enable_memory_efficient else len(image)
            
            for i in range(0, len(image), batch_size):
                batch_images = image[i:i+batch_size]
                
                for img_tensor in batch_images:
                    try:
                        orig_image = tensor2pil(img_tensor)
                        w, h = orig_image.size
                        
                        # æ™ºèƒ½resize
                        resized_image = resize_image(orig_image, max_resolution)
                        im_tensor = transform_image(resized_image).unsqueeze(0)
                        im_tensor = im_tensor.to(device)
                        
                        # æ¨ç†
                        with torch.no_grad():
                            result = birefnet(im_tensor)[-1].sigmoid()
                            
                            # ç«‹å³ç§»åˆ°CPUé‡Šæ”¾GPUæ˜¾å­˜
                            result = result.cpu()
                            
                            # æ¸…ç†ä¸­é—´å¼ é‡
                            del im_tensor
                            if device.startswith('cuda'):
                                torch.cuda.empty_cache()
                        
                        # åå¤„ç†
                        result = torch.squeeze(F.interpolate(result, size=(h, w), mode='bilinear', align_corners=False))
                        ma = torch.max(result)
                        mi = torch.min(result)
                        result = (result - mi) / (ma - mi + 1e-8)  # æ·»åŠ å°å€¼é¿å…é™¤é›¶
                        
                        im_array = (result * 255).data.numpy().astype(np.uint8)
                        pil_im = Image.fromarray(np.squeeze(im_array))
                        
                        # ç”Ÿæˆæœ€ç»ˆå›¾åƒ
                        if background_color_name == 'transparency':
                            color = (0, 0, 0, 0)
                            mode = "RGBA"
                        else:
                            color = background_color_name
                            mode = "RGB"
                        
                        new_im = Image.new(mode, pil_im.size, color)
                        new_im.paste(orig_image, mask=pil_im)
                        
                        new_im_tensor = pil2tensor(new_im)
                        pil_im_tensor = pil2tensor(pil_im)
                        
                        processed_images.append(new_im_tensor)
                        processed_masks.append(pil_im_tensor)
                        
                        # æ¸…ç†ä¸­é—´å˜é‡
                        del result, im_array, pil_im, new_im
                        
                    except Exception as e:
                        print(f"\033[91må¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}\033[0m")
                        # åœ¨å‡ºé”™æ—¶ä¹Ÿè¦æ¸…ç†æ˜¾å­˜
                        if device.startswith('cuda'):
                            torch.cuda.empty_cache()
                        raise e
                
                # æ‰¹å¤„ç†é—´æ¸…ç†
                if enable_memory_efficient and device.startswith('cuda'):
                    torch.cuda.empty_cache()
                    gc.collect()

            new_ims = torch.cat(processed_images, dim=0)
            new_masks = torch.cat(processed_masks, dim=0)

            return new_ims, new_masks
            
        except Exception as e:
            print(f"\033[91mBiRefNetå¤„ç†å¤±è´¥: {str(e)}\033[0m")
            # å‘ç”Ÿé”™è¯¯æ—¶æ¸…ç†æ˜¾å­˜
            if device.startswith('cuda'):
                torch.cuda.empty_cache()
            gc.collect()
            raise e


# æ·»åŠ æ¸…ç†å‡½æ•°èŠ‚ç‚¹
class BiRefNet_ClearCache:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "trigger": ("BOOLEAN", {"default": True}),
            }
        }

    RETURN_TYPES = ("BOOLEAN",)
    RETURN_NAMES = ("success",)
    FUNCTION = "clear_cache"
    CATEGORY = "ğŸ”¥BiRefNet"
    
    def clear_cache(self, trigger):
        """æ‰‹åŠ¨æ¸…ç†BiRefNetæ¨¡å‹ç¼“å­˜"""
        if trigger:
            clear_model_cache()
            print("\033[92mBiRefNetæ¨¡å‹ç¼“å­˜å·²æ¸…ç†\033[0m")
        return (True,)


NODE_CLASS_MAPPINGS = {
    "BiRefNet_Hugo": BiRefNet_Hugo,
    "BiRefNet_ClearCache": BiRefNet_ClearCache
}

# A dictionary that contains the friendly/humanly readable titles for the nodes
NODE_DISPLAY_NAME_MAPPINGS = {
    "BiRefNet_Hugo": "ğŸ”¥BiRefNet",
    "BiRefNet_ClearCache": "ğŸ”¥BiRefNetæ¸…ç†ç¼“å­˜"
}
